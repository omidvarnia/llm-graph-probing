# -----------------------------------------------------------------------------
# Model options (set model_name to one of these). Use valid Hugging Face IDs.
#
# GPT-2 family (HF IDs):
#   - gpt2
#   - gpt2-medium
#   - gpt2-large
#
# Pythia family (HF IDs):
#   - EleutherAI/pythia-160m
#   - EleutherAI/pythia-410m
#   - EleutherAI/pythia-1.4b
#   - EleutherAI/pythia-2.8b
#   - EleutherAI/pythia-6.9b
#   - EleutherAI/pythia-12b
#
# Qwen base models (HF IDs):
#   - Qwen/Qwen1.5-0.5B
#   - Qwen/Qwen2-0.5B
#   - Qwen/Qwen2.5-0.5B
#   - Qwen/Qwen2.5-1.5B
#   - Qwen/Qwen2.5-3B
#   - Qwen/Qwen2.5-7B
#   - Qwen/Qwen2.5-14B
#
# Qwen instruct/chat models (HF IDs):
#   - Qwen/Qwen2.5-0.5B-Instruct
#   - Qwen/Qwen2.5-1.5B-Instruct
#   - Qwen/Qwen2.5-3B-Instruct
#   - Qwen/Qwen2.5-7B-Instruct
#   - Qwen/Qwen2.5-14B-Instruct
#
# Notes:
# - A "valid Hugging Face ID" is the exact repository name (org/model) that
#   can be passed to Transformers' AutoTokenizer/AutoModelForCausalLM.
# - You can set model_name directly to any HF ID above. GPT-2 names are already valid IDs.
# - For new models, either set model_name to the HF ID, or add an alias mapping
#   in utils/constants.py:hf_model_name_map if you prefer short names.
# - Chat models are auto-wrapped with chat templates when applicable.
# -----------------------------------------------------------------------------

common:
  dataset_name: truthfulqa              # Dataset name (truthfulqa, halueval, medhallu, helm)
  model_name: Qwen/Qwen2.5-0.5B    # Hugging Face model ID to load
  ckpt_step: -1                         # Checkpoint step (-1 uses main checkpoint)
  layer_list: all                       # Layers to process: 'all' for automatic detection, or comma-separated list like '5,6,7,8,9,10,11'
  density: 0.05                         # Network density threshold (0.0-1.0) for FC sparsification
  steps: "1,2,3,4"                   # Pipeline steps to execute: comma-separated list (e.g., "1", "1,2", "1,3,4", "3,5"). If not starting from 1, previous step results must exist or error will be raised
  enable_cleanup: false                 # When true, delete Step 3+ outputs (reports/saves/runs) before starting; when false, preserve all existing results
  # ===== Probe Architecture (Reference uses GCN with 3 layers) =====
  # Option 1: GCN with 3 layers (RECOMMENDED - matches reference)
  #   num_layers: 3
  #   Note: Requires torch_scatter compiled with ROCm. If unavailable, will fail at Step 3 (train).
  #         Compile from source: git clone https://github.com/rusty1s/pytorch_scatter && 
  #                             cd pytorch_scatter && pip install -e . --no-build-isolation
  # Option 2: MLP (no graph structure, fallback)
  #   num_layers: 0  (means num_layers < 0 for MLP with abs(num_layers) hidden layers)
  #   num_layers: -2  (2 hidden layers in MLP)
  num_layers: 3                         # GCN depth (3 layers = reference config); use -N for MLP with N hidden layers
  hidden_channels: 32                   # GNN hidden size (same as reference)
  gpu_id: 0                             # GPU index to use
  from_sparse_data: true                # Expect sparse FC inputs
  early_stop_patience: 20               # Early stopping patience (epochs)
  aggregate_layers: false               # Also save combined FC across provided layers
  project_dir: /u/aomidvarnia/GIT_repositories/llm-graph-probing  # Root of this repo
  main_dir: /ptmp/aomidvarnia/analysis_results/llm_graph          # Root output directory

hallucination:
  batch_size: 16                        # LLM forward batch size
  probe_input: corr                     # Probe input type: corr or activation (reference uses corr)
  eval_batch_size: 32                   # Eval batch size for probe
  learning_rate: 0.001                  # Probe optimizer learning rate (reference uses 0.001)
  num_epochs: 100                       # Max probe training epochs (reference uses 10, but we use 100 with early stopping)
  label_smoothing: null                 # Label smoothing factor (null=disabled, 0.0-1.0=enabled, higher=more smoothing)
  gradient_clip: 1.0                    # Max gradient norm for clipping (null=disabled, positive=enabled, prevents exploding gradients)
  seed: null                            # Random seed for reproducibility (null=auto-generate, integer=fixed split, same seed ensures identical train/test split across all layers)

neuropathology:
  disease_pattern: epilepsy_like        # Disease preset (epilepsy_like, dementia_like, autism_like)
  num_clusters: 8                       # Number of functional modules
  within_scale: 1.2                     # Within-module correlation scaling (>1 increases segregation)
  between_scale: 0.7                    # Between-module correlation scaling (<1 increases integration loss)
  rewiring_prob: 0.15                   # Edge rewiring probability
  distance_threshold: null              # Optional index-distance threshold for attenuation (null disables)
